#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æœŸåˆŠæ‘˜è¦ç”Ÿæˆå™¨

åŠŸèƒ½ï¼š
- è‡ªåŠ¨åŒæ­¥ awesome-english-ebooks æœŸåˆŠä»“åº“
- ä»EPUBæ–‡ä»¶æå–æ–‡æœ¬å†…å®¹
- ä½¿ç”¨AIç”ŸæˆæœŸåˆŠæ–‡ç« æ‘˜è¦
- åˆ›å»ºJSONæ‘˜è¦æ–‡ä»¶å’ŒWordæŠ¥å‘Š

æ”¯æŒçš„æœŸåˆŠï¼š
- ç»æµå­¦äºº (The Economist)
- çº½çº¦å®¢ (The New Yorker)  
- å¤§è¥¿æ´‹æœˆåˆŠ (The Atlantic)
- è¿çº¿ (Wired)

ä½¿ç”¨æ–¹æ³•ï¼š
    python summarizer.py [magazine_type] [-o output_dir] [--skip-git-check]
    
    ç¤ºä¾‹ï¼š
    python summarizer.py economist          # å¤„ç†ç»æµå­¦äºº
    python summarizer.py 1                  # å¤„ç†ç¬¬1ä¸ªæ‚å¿—ï¼ˆç»æµå­¦äººï¼‰
    python summarizer.py                    # äº¤äº’å¼é€‰æ‹©

è¾“å‡ºï¼š
    åœ¨ æ‘˜è¦æ±‡æ€»/ ç›®å½•ä¸‹ç”Ÿæˆï¼š
    - JSONæ‘˜è¦æ–‡ä»¶
    - Wordæ±‡æ€»æŠ¥å‘Š
"""

import os
import glob
import json
import re
import sys
import argparse
import subprocess
import docx
from docx.oxml.ns import qn
import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup
from openai import OpenAI
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# Gitä»“åº“è·¯å¾„ - ä½¿ç”¨ç›¸å¯¹è·¯å¾„
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
# ä» functions/ å‘ä¸Šä¸€çº§åˆ° å†…å®¹ç”Ÿæˆå™¨/ï¼Œå†å‘ä¸Šä¸€çº§åˆ° æœŸåˆŠç³»åˆ—å†…å®¹ç”Ÿæˆ/ï¼Œç„¶åè¿›å…¥ awesome-english-ebooks/
REPO_PATH = os.path.join(os.path.dirname(os.path.dirname(SCRIPT_DIR)), "awesome-english-ebooks")

# æ‚å¿—é…ç½®ä¿¡æ¯
MAGAZINE_CONFIG = {
    "economist": {"name": "The Economist", "base_dir": "01_economist", "folder_pattern": "te_*", "file_pattern": "*.epub", "title": "ç»æµå­¦äºº"},
    "new_yorker": {"name": "The New Yorker", "base_dir": "02_new_yorker", "folder_pattern": "20*", "file_pattern": "*.epub", "title": "çº½çº¦å®¢"},
    "atlantic": {"name": "The Atlantic", "base_dir": "04_atlantic", "folder_pattern": "20*", "file_pattern": "*.epub", "title": "å¤§è¥¿æ´‹æœˆåˆŠ"},
    "wired": {"name": "Wired", "base_dir": "05_wired", "folder_pattern": "20*", "file_pattern": "*.epub", "title": "è¿çº¿"}
}

# ä½¿ç”¨OpenRouter APIåˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
client = OpenAI(
    base_url=os.getenv("OPENROUTER_BASE_URL"),
    api_key=os.getenv("OPENROUTER_API_KEY")
)

def check_and_update_repo():
    """æ£€æŸ¥å¹¶æ›´æ–°æˆ–å…‹éš† awesome-english-ebooks æœŸåˆŠä»“åº“"""
    repo_url = "https://github.com/hehonghui/awesome-english-ebooks.git"
    
    def run_git_cmd(cmd, timeout=30, cwd=None):
        """è¿è¡ŒGitå‘½ä»¤çš„è¾…åŠ©å‡½æ•°"""
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=timeout, cwd=cwd or REPO_PATH)
            return result.returncode == 0, result.stdout.strip(), result.stderr
        except subprocess.TimeoutExpired:
            return False, "", "æ“ä½œè¶…æ—¶"
        except Exception as e:
            return False, "", str(e)
    
    print(f"æ­£åœ¨æ£€æŸ¥æœŸåˆŠä»“åº“çŠ¶æ€ ({REPO_PATH})...")
    
    # æ£€æŸ¥ä»“åº“æ˜¯å¦å­˜åœ¨
    if not os.path.exists(REPO_PATH):
        print("æœªå‘ç°æœŸåˆŠä»“åº“ï¼Œæ­£åœ¨å…‹éš†...")
        parent_dir = os.path.dirname(REPO_PATH)
        os.makedirs(parent_dir, exist_ok=True)
        
        success, output, error = run_git_cmd(['git', 'clone', repo_url, "awesome-english-ebooks"], 120, parent_dir)
        if success:
            print("âœ“ æœŸåˆŠä»“åº“å…‹éš†æˆåŠŸ!")
            return True
        else:
            print(f"âœ— æœŸåˆŠä»“åº“å…‹éš†å¤±è´¥: {error}")
            return False
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆGitä»“åº“
    if not os.path.exists(os.path.join(REPO_PATH, '.git')):
        print(f"âš  å‘ç° {REPO_PATH} ç›®å½•ï¼Œä½†ä¸æ˜¯æœ‰æ•ˆçš„Gitä»“åº“")
        return False
    
    # è·å–è¿œç¨‹æ›´æ–°ä¿¡æ¯
    success, _, error = run_git_cmd(['git', 'fetch'])
    if not success:
        print(f"è·å–è¿œç¨‹ä¿¡æ¯å¤±è´¥: {error}")
        return False
    
    # è·å–å½“å‰åˆ†æ”¯å’Œæäº¤ä¿¡æ¯
    success, current_branch, _ = run_git_cmd(['git', 'rev-parse', '--abbrev-ref', 'HEAD'])
    if not success:
        print("è·å–åˆ†æ”¯ä¿¡æ¯å¤±è´¥")
        return False
    
    # æ¯”è¾ƒæœ¬åœ°å’Œè¿œç¨‹æäº¤
    success, local_commit, _ = run_git_cmd(['git', 'rev-parse', 'HEAD'])
    success2, remote_commit, _ = run_git_cmd(['git', 'rev-parse', f'origin/{current_branch}'])
    
    if not (success and success2):
        print("è·å–æäº¤ä¿¡æ¯å¤±è´¥")
        return False
    
    if local_commit == remote_commit:
        print("âœ“ æœŸåˆŠä»“åº“å·²ç»æ˜¯æœ€æ–°ç‰ˆæœ¬")
        return True
    
    # æ£€æŸ¥å¹¶æ›´æ–°
    success, status_output, _ = run_git_cmd(['git', 'status', '-uno'])
    if not success:
        print("æ£€æŸ¥çŠ¶æ€å¤±è´¥")
        return False
    
    if "Your branch is behind" in status_output:
        print("å‘ç°æœŸåˆŠä»“åº“æœ‰æ›´æ–°ï¼Œæ­£åœ¨æ‰§è¡Œ git pull...")
        success, output, error = run_git_cmd(['git', 'pull', '--stat'], 60)
        if success:
            print("âœ“ æœŸåˆŠä»“åº“æ›´æ–°æˆåŠŸ!")
            
            # æ˜¾ç¤ºæ›´æ–°çš„æœŸåˆŠæ–‡ä»¶å¤¹
            if output.strip():
                updated_folders = set()
                for line in output.strip().split('\n'):
                    if '|' in line and ('+' in line or '-' in line):
                        file_path = line.split('|')[0].strip()
                        if file_path and not file_path.startswith(('create mode', 'delete mode')):
                            path_parts = file_path.split('/')
                            if len(path_parts) >= 2:
                                folder_path = '/'.join(path_parts[:2])
                                updated_folders.add(folder_path)
                
                if updated_folders:
                    print("ğŸ“‚ æ›´æ–°çš„æœŸåˆŠæ–‡ä»¶å¤¹:")
                    for folder in sorted(updated_folders):
                        print(f"   ğŸ“ {folder}")
            
            return True
        else:
            print(f"âœ— æœŸåˆŠä»“åº“æ›´æ–°å¤±è´¥: {error}")
            return False
    elif "have diverged" in status_output:
        print("æ£€æµ‹åˆ°åˆ†æ”¯åˆ†å‰ï¼Œæ­£åœ¨å¼ºåˆ¶æ›´æ–°åˆ°è¿œç¨‹ç‰ˆæœ¬...")
        success, _, error = run_git_cmd(['git', 'reset', '--hard', f'origin/{current_branch}'])
        if success:
            print("âœ“ æœŸåˆŠä»“åº“å·²å¼ºåˆ¶æ›´æ–°åˆ°è¿œç¨‹ç‰ˆæœ¬!")
            return True
        else:
            print(f"âœ— å¼ºåˆ¶æ›´æ–°å¤±è´¥: {error}")
            return False
    else:
        print("âœ“ æ£€æµ‹åˆ°æ›´æ–°ä½†çŠ¶æ€å¤æ‚ï¼Œå»ºè®®æ‰‹åŠ¨å¤„ç†")
        return False

def extract_text_from_epub(epub_path):
    """ä½¿ç”¨ebooklibå’ŒBeautifulSoupä»EPUBæ–‡ä»¶ä¸­æå–æ–‡æœ¬ã€‚"""
    try:
        text_parts = []
        book = epub.read_epub(epub_path)
        
        for item in book.get_items():
            if item.get_type() == ebooklib.ITEM_DOCUMENT:
                try:
                    content = item.get_content().decode('utf-8')
                    soup = BeautifulSoup(content, 'html.parser')
                    
                    # ç§»é™¤scriptå’Œstyleæ ‡ç­¾
                    for script in soup(["script", "style"]):
                        script.decompose()
                    
                    # æå–æ–‡æœ¬
                    text = soup.get_text(' ', strip=True)
                    if text:
                        text_parts.append(text)
                except Exception as e:
                    print(f"  å¤„ç†EPUBç« èŠ‚æ—¶å‡ºé”™: {e}")
                    continue
        
        return '\n\n'.join(text_parts)
    except Exception as e:
        print(f"ä»{epub_path}æå–æ–‡æœ¬æ—¶å‡ºé”™: {e}")
        return ""

def parse_date_from_filename(filename, magazine_type):
    """ä»æ–‡ä»¶åä¸­è§£ææ—¥æœŸ"""
    match = re.search(r'(\d{4})\.(\d{2})\.(\d{2})', filename)
    return f"{match.group(1)}-{match.group(2)}-{match.group(3)}" if match else None

def summarize_with_llm(epub_text, journal_name, publication_date):
    """ä½¿ç”¨LLMç”Ÿæˆæ‚å¿—æ‘˜è¦"""
    prompt = f"""è¯·åˆ†æä»¥ä¸‹ã€Š{journal_name}ã€‹æ‚å¿—ï¼ˆå‘è¡Œæ—¥æœŸï¼š{publication_date}ï¼‰çš„å†…å®¹ã€‚
è¯·æå–å‡ºæ‚å¿—ä¸­æ¯ç¯‡æ–‡ç« çš„ä¸»è¦å†…å®¹ï¼Œæ¯ç¯‡æ–‡ç« çš„æ€»ç»“çº¦300å­—ï¼Œç”¨ç®€ä½“ä¸­æ–‡è¾“å‡ºã€‚

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONç»“æ„è¾“å‡ºï¼š
{{
  "journal_name": "{journal_name}",
  "publication_date": "{publication_date}",
  "articles": [
    {{
      "title": "æ ‡é¢˜ç¤ºä¾‹1",
      "Chinese_title": "å¯¹åº”çš„ä¸­æ–‡æ ‡é¢˜ç¤ºä¾‹1",
      "summary": "æ–‡ç« æ‘˜è¦å†…å®¹..."
    }}
  ]
}}

é‡è¦è¯´æ˜ï¼š
1. "publication_date" å­—æ®µå¿…é¡»ä¸¥æ ¼ä½¿ç”¨ YYYY-MM-DD æ ¼å¼ï¼ˆå¦‚ 2024-01-02ï¼‰
2. ä¸è¦ä½¿ç”¨æœˆä»½åç§°ï¼ˆå¦‚ January/February 2024ï¼‰æˆ–å…¶ä»–æ—¥æœŸæ ¼å¼
3. è¯·ä½¿ç”¨ä¼ å…¥çš„ publication_date å‚æ•°å€¼ï¼š{publication_date}

ä»¥ä¸‹æ˜¯æ‚å¿—å†…å®¹ï¼š
{epub_text}

è¯·åªè¾“å‡ºæœ‰æ•ˆçš„JSONæ ¼å¼ã€‚"""

    try:
        response = client.chat.completions.create(
            model=os.getenv("OPENROUTER_Gemini_2.5_Pro"),
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"},
            max_tokens=30000,
            temperature=0.3
        )
        
        content = response.choices[0].message.content.strip()
        if not content:
            raise ValueError("LLMè¿”å›äº†ç©ºå“åº”")
            
        # æ¸…ç†å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
        if content.startswith('```json'):
            content = content[7:]
        if content.endswith('```'):
            content = content[:-3]
        content = content.strip()
        
        data = json.loads(content)
        
        # éªŒè¯å“åº”æ ¼å¼
        required_keys = ('journal_name', 'publication_date', 'articles')
        if not (isinstance(data, dict) and all(k in data for k in required_keys) and
               isinstance(data.get('articles'), list) and len(data['articles']) > 0):
            raise ValueError("LLMå“åº”æ ¼å¼æ— æ•ˆ")
            
        return data
        
    except (json.JSONDecodeError, ValueError) as e:
        print(f"å¤„ç†LLMå“åº”æ—¶å‡ºé”™: {e}")
        print(f"å“åº”å†…å®¹å‰500å­—ç¬¦: {content[:500] if 'content' in locals() else 'None'}")
    except Exception as e:
        print(f"è°ƒç”¨LLMæ—¶å‡ºé”™: {e}")
    return None

def save_json_summary(summary_data, output_dir, filename):
    """å°†æ‘˜è¦ä¿å­˜ä¸ºJSONæ–‡ä»¶ã€‚"""
    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, filename)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(summary_data, f, ensure_ascii=False, indent=2)
    
    return output_path

def create_docx_report(json_files, output_path, magazine_title="æ‚å¿—"):
    """åˆ›å»ºWordæŠ¥å‘Š"""
    # æŒ‰å‡ºç‰ˆæ—¥æœŸæ’åºæ–‡ä»¶
    sorted_files = sorted(json_files, key=lambda x: json.load(open(x, 'r', encoding='utf-8')).get('publication_date', ''))
    
    doc = docx.Document()
    
    # è®¾ç½®å­—ä½“æ ·å¼
    def set_font(style_name, font_name='SimSun', font_size=10.5):
        style = doc.styles[style_name]
        style.font.name = font_name
        style.font.size = docx.shared.Pt(font_size)
        style.font._element.rPr.rFonts.set(qn('w:eastAsia'), font_name)
    
    for style, size in [('Normal', 10.5), ('Heading 1', 16), ('Heading 2', 14), ('Heading 3', 12)]:
        set_font(style, 'SimSun', size)
    
    doc.add_heading(f'ã€Š{magazine_title}ã€‹æœŸåˆŠæ‘˜è¦æ±‡æ€»', 0)
    
    # å¤„ç†æ¯ä¸ªJSONæ–‡ä»¶
    for json_file in sorted_files:
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            doc.add_heading(f"{data.get('journal_name', 'æœªçŸ¥')} ({data.get('publication_date', 'æœªçŸ¥')})", 1)
            
            for article in data.get('articles', []):
                english_title = article.get('title', 'æœªçŸ¥æ ‡é¢˜')
                chinese_title = article.get('Chinese_title', '')
                full_title = f"{chinese_title} ({english_title})" if chinese_title else english_title
                
                doc.add_heading(full_title, 2)
                doc.add_paragraph(article.get('summary', 'æ— å¯ç”¨æ‘˜è¦'))
            
            doc.add_page_break()
            
        except Exception as e:
            print(f"å¤„ç†{json_file}æ—¶å‡ºé”™: {e}")
    
    doc.save(output_path)
    print(f"WordæŠ¥å‘Šå·²ä¿å­˜è‡³{output_path}")

def _get_magazine_paths(magazine_type, base_output_dir):
    """è·å–æ‚å¿—ç›¸å…³çš„è·¯å¾„å’Œæ–‡ä»¶ä¿¡æ¯"""
    config = MAGAZINE_CONFIG[magazine_type]
    base_path = os.path.join(REPO_PATH, config['base_dir'])
    
    # ä¿®å¤è¾“å‡ºç›®å½•è·¯å¾„è®¡ç®—
    if os.path.isabs(base_output_dir):
        magazine_output_dir = os.path.join(base_output_dir, config['title'])
    else:
        # ä¿®å¤è·¯å¾„ï¼šä»functionsç›®å½•å‘ä¸Šä¸€çº§åˆ°é¡¹ç›®æ ¹ç›®å½•
        project_root = os.path.dirname(SCRIPT_DIR)  # å‘ä¸Šä¸€çº§åˆ°é¡¹ç›®æ ¹ç›®å½•
        magazine_output_dir = os.path.join(project_root, base_output_dir, config['title'])
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(magazine_output_dir, exist_ok=True)
    
    # æŸ¥æ‰¾æ‰€æœ‰ç¬¦åˆæ¨¡å¼çš„ç›®å½•
    magazine_dirs = glob.glob(os.path.join(base_path, config['folder_pattern']))
    
    # æ”¶é›†æ‰€æœ‰EPUBæ–‡ä»¶
    all_epub_files = []
    valid_epub_files = []
    valid_folders = []
    
    # ç»Ÿè®¡æœ‰æ•ˆçš„æ–‡ä»¶å¤¹å’ŒEPUBæ–‡ä»¶
    for dir_path in magazine_dirs:
        if os.path.isdir(dir_path):
            epub_files = glob.glob(os.path.join(dir_path, config['file_pattern']))
            if epub_files:  # åªç»Ÿè®¡åŒ…å«EPUBæ–‡ä»¶çš„æ–‡ä»¶å¤¹
                valid_folders.append(dir_path)
                all_epub_files.extend(epub_files)
    
    # å¤„ç†EPUBæ–‡ä»¶ï¼Œæå–æ—¥æœŸå’Œç”Ÿæˆé¢„æœŸçš„JSONè·¯å¾„ï¼Œå»é‡å¤„ç†
    epub_info = []
    unique_dates = set()
    seen_dates = {}  # ç”¨äºè·Ÿè¸ªå·²è§è¿‡çš„æ—¥æœŸ
    
    for epub_path in all_epub_files:
        filename = os.path.basename(epub_path)
        publication_date = parse_date_from_filename(filename, magazine_type)
        if publication_date:
            unique_dates.add(publication_date)
            # å¯¹é‡å¤æ—¥æœŸè¿›è¡Œå»é‡ï¼Œä¿ç•™ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„æ–‡ä»¶
            if publication_date not in seen_dates:
                json_filename = f"{config['name']}_{publication_date.replace('-', '')}.json"
                json_path = os.path.join(magazine_output_dir, json_filename)
                epub_info.append({
                    'epub_path': epub_path,
                    'json_path': json_path,
                    'publication_date': publication_date,
                    'filename': filename
                })
                valid_epub_files.append(epub_path)
                seen_dates[publication_date] = True
    
    # è·å–å·²å­˜åœ¨çš„JSONæ–‡ä»¶
    existing_json_files = []
    if os.path.exists(magazine_output_dir):
        existing_json_files = [f for f in glob.glob(os.path.join(magazine_output_dir, "*.json")) 
                              if not f.endswith('æ‘˜è¦æ±‡æ€».docx')]  # æ’é™¤Wordæ–‡æ¡£
    existing_json_set = set(existing_json_files)
    
    return {
        'config': config,
        'magazine_dirs': magazine_dirs,
        'all_epub_files': all_epub_files,
        'valid_epub_files': valid_epub_files,
        'valid_folders': valid_folders,
        'epub_info': epub_info,
        'unique_dates': unique_dates,
        'existing_json_files': existing_json_files,
        'existing_json_set': existing_json_set,
        'magazine_output_dir': magazine_output_dir
    }

def count_magazine_files(magazine_type, base_output_dir="æ‘˜è¦æ±‡æ€»"):
    """ç»Ÿè®¡æ‚å¿—æ–‡ä»¶æ•°é‡"""
    try:
        paths = _get_magazine_paths(magazine_type, base_output_dir)
        existing_expected = {info['json_path'] for info in paths['epub_info']} & set(paths['existing_json_files'])
        
        return {
            "total_folders": len(paths['valid_folders']),
            "unique_dates": len(paths['unique_dates']),
            "existing_json": len(paths['existing_json_files']),
            "to_process": len(paths['valid_folders']) - len(existing_expected)
        }
    except Exception as e:
        print(f"ç»Ÿè®¡æ‚å¿—æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return {"total_folders": 0, "unique_dates": 0, "existing_json": 0, "to_process": 0}

def process_magazine(magazine_type, base_output_dir="æ‘˜è¦æ±‡æ€»"):
    """å¤„ç†æŒ‡å®šç±»å‹çš„æ‚å¿—"""
    if magazine_type not in MAGAZINE_CONFIG:
        print(f"é”™è¯¯ï¼šæœªçŸ¥çš„æ‚å¿—ç±»å‹ '{magazine_type}'")
        return
    
    config = MAGAZINE_CONFIG[magazine_type]
    print(f"\n{'-' * 20} å¼€å§‹å¤„ç† {config['title']} {'-' * 20}")
    
    try:
        paths = _get_magazine_paths(magazine_type, base_output_dir)
        existing_expected = {info['json_path'] for info in paths['epub_info']} & paths['existing_json_set']
        needs_processing = [info for info in paths['epub_info'] if info['json_path'] not in paths['existing_json_set']]
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print(f"\n{'=' * 60}")
        print(f"ã€{config['name']}æ‘˜è¦ç»Ÿè®¡ä¿¡æ¯ã€‘")
        print(f"  - æ€»æœŸæ•°: {len(paths['valid_folders'])} æœŸ")
        print(f"  - å·²å®Œæˆæ‘˜è¦: {len(existing_expected)} æœŸ")
        print(f"  - å¾…å¤„ç†: {len(needs_processing)} æœŸ")
        print("=" * 60 + "\n")
        
        if not needs_processing:
            print("æ‰€æœ‰æ–‡ä»¶å·²ç»å¤„ç†å®Œæ¯•ï¼Œæ— éœ€è¿›è¡Œæ–°çš„æ‘˜è¦å¤„ç†\n")
            # å¦‚æœæ²¡æœ‰éœ€è¦å¤„ç†çš„æ–‡ä»¶ï¼Œä»ç„¶ç”ŸæˆWordæŠ¥å‘Šï¼ˆå¦‚æœæœ‰ç°æœ‰çš„JSONæ–‡ä»¶ï¼‰
            word_report_success = False
            if existing_expected:
                try:
                    docx_filename = f"ã€Š{config['title']}ã€‹æ‘˜è¦æ±‡æ€».docx"
                    docx_path = os.path.join(paths['magazine_output_dir'], docx_filename)
                    create_docx_report(list(existing_expected), docx_path, config['title'])
                    print(f"WordæŠ¥å‘Šå·²ä¿å­˜è‡³{docx_path}")
                    word_report_success = True
                except Exception as e:
                    print(f"åˆ›å»ºWordæ–‡æ¡£æ—¶å‡ºé”™: {e}")
            
            return {
                'total': 0,
                'successful': 0,
                'failed': 0,
                'word_report': word_report_success,
                'all_successful': True,
                'partially_successful': True,
                'already_completed': True
            }
        else:
            print("å¼€å§‹è¿›è¡Œæ‘˜è¦å¤„ç†...\n")
        
        # å¤„ç†éœ€è¦ç”Ÿæˆæ‘˜è¦çš„æ–‡ä»¶
        json_files = list(existing_expected)
        successful_count = 0
        failed_count = 0
        
        for i, info in enumerate(needs_processing):
            try:
                print(f"[{i+1}/{len(needs_processing)}] æ­£åœ¨å¤„ç†{info['filename']}...")
                
                epub_text = extract_text_from_epub(info['epub_path'])
                if not epub_text:
                    print(f"  ä»{info['epub_path']}æå–æ–‡æœ¬å¤±è´¥ï¼Œè·³è¿‡")
                    failed_count += 1
                    continue
                
                print("  æ­£åœ¨ä½¿ç”¨Gemini 2.5 Proè¿›è¡Œæ‘˜è¦...")
                summary_data = summarize_with_llm(epub_text, config['name'], info['publication_date'])
                
                if summary_data:
                    save_json_summary(summary_data, paths['magazine_output_dir'], os.path.basename(info['json_path']))
                    print(f"  æ‘˜è¦å·²ä¿å­˜è‡³{info['json_path']}")
                    json_files.append(info['json_path'])
                    successful_count += 1
                else:
                    print(f"  ä¸º{info['filename']}ç”Ÿæˆæ‘˜è¦å¤±è´¥ï¼Œè·³è¿‡")
                    failed_count += 1
            except Exception as e:
                print(f"  å¤„ç†{info.get('filename', 'æœªçŸ¥æ–‡ä»¶')}æ—¶å‡ºé”™: {e}")
                failed_count += 1
                continue
        
        # åˆ›å»ºWordæŠ¥å‘Š
        word_report_success = False
        if json_files:
            try:
                docx_filename = f"ã€Š{config['title']}ã€‹æ‘˜è¦æ±‡æ€».docx"
                docx_path = os.path.join(paths['magazine_output_dir'], docx_filename)
                create_docx_report(json_files, docx_path, config['title'])
                print(f"WordæŠ¥å‘Šå·²ä¿å­˜è‡³{docx_path}")
                word_report_success = True
            except Exception as e:
                print(f"åˆ›å»ºWordæ–‡æ¡£æ—¶å‡ºé”™: {e}")
        
        # æ‰“å°å¤„ç†ç»“æœæ±‡æ€»
        total_processed = len(needs_processing)
        print(f"\n{'='*50}")
        print(f"ğŸ“Š {config['title']} å¤„ç†ç»“æœæ±‡æ€»:")
        print(f"  ğŸ“ æ€»è®¡å¤„ç†: {total_processed} ç¯‡")
        print(f"  âœ… æˆåŠŸç”Ÿæˆ: {successful_count} ç¯‡")
        print(f"  âŒ å¤„ç†å¤±è´¥: {failed_count} ç¯‡")
        if word_report_success:
            print(f"  ğŸ“„ WordæŠ¥å‘Š: å·²ç”Ÿæˆ")
        
        if failed_count > 0:
            print(f"\nâš ï¸  éƒ¨åˆ†æ–‡ä»¶å¤„ç†å¤±è´¥ï¼Œè¿™é€šå¸¸æ˜¯ç”±äºLLMå“åº”ä¸ç¨³å®šå¯¼è‡´çš„ã€‚")
            print(f"   å»ºè®®é‡æ–°è¿è¡Œç¨‹åºå¤„ç†å¤±è´¥çš„æ–‡ä»¶ã€‚")
        
        print(f"{'='*50}")
        
        # è¿”å›å¤„ç†ç»“æœ
        return {
            'total': total_processed,
            'successful': successful_count,
            'failed': failed_count,
            'word_report': word_report_success,
            'all_successful': failed_count == 0,
            'partially_successful': successful_count > 0
        }
        
    except Exception as e:
        print(f"å¤„ç†æ‚å¿—{magazine_type}æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return {
            'total': 0,
            'successful': 0,
            'failed': 0,
            'word_report': False,
            'all_successful': False,
            'partially_successful': False,
            'error': str(e)
        }

def main():
    """ä¸»å‡½æ•°ï¼Œå¤„ç†å‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="æ‚å¿—æ‘˜è¦ç”Ÿæˆå·¥å…·")
    
    # æ·»åŠ æ‚å¿—ç±»å‹å‚æ•°
    parser.add_argument(
        "magazine", 
        nargs="?",  # å¯é€‰å‚æ•°
        choices=list(MAGAZINE_CONFIG.keys()) + [str(i) for i in range(1, len(MAGAZINE_CONFIG) + 1)],
        help="è¦å¤„ç†çš„æ‚å¿—ç±»å‹æˆ–ç¼–å·"
    )
    
    # æ·»åŠ è¾“å‡ºç›®å½•å‚æ•°
    parser.add_argument(
        "-o", "--output",
        default="æ‘˜è¦æ±‡æ€»",
        help="æ‘˜è¦è¾“å‡ºçš„åŸºç¡€ç›®å½• (é»˜è®¤: æ‘˜è¦æ±‡æ€»)"
    )
    
    # æ·»åŠ è·³è¿‡Gitæ£€æŸ¥çš„é€‰é¡¹
    parser.add_argument(
        "--skip-git-check",
        action="store_true",
        help="è·³è¿‡æœŸåˆŠä»“åº“æ›´æ–°æ£€æŸ¥"
    )
    
    args = parser.parse_args()
    
    # æ£€æŸ¥å¹¶æ›´æ–°æœŸåˆŠä»“åº“ï¼ˆé™¤éç”¨æˆ·é€‰æ‹©è·³è¿‡ï¼‰
    if not args.skip_git_check:
        print("=" * 60)
        print("ã€æœŸåˆŠä»“åº“æ›´æ–°æ£€æŸ¥ã€‘")
        print("=" * 60)
        
        # ä¿å­˜å½“å‰å·¥ä½œç›®å½•
        original_cwd = os.getcwd()
        
        try:
            if not check_and_update_repo():
                print("\nè­¦å‘Šï¼šæœŸåˆŠä»“åº“æ›´æ–°æ£€æŸ¥å¤±è´¥ï¼Œä½†ç¨‹åºå°†ç»§ç»­è¿è¡Œ...")
                print("å¦‚æœéœ€è¦è·³è¿‡æ£€æŸ¥ï¼Œè¯·ä½¿ç”¨ --skip-git-check å‚æ•°\n")
            else:
                print()
        finally:
            # æ¢å¤åŸå§‹å·¥ä½œç›®å½•
            os.chdir(original_cwd)
    else:
        print("å·²è·³è¿‡æœŸåˆŠä»“åº“æ›´æ–°æ£€æŸ¥\n")
    
    # æ˜¾ç¤ºæ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯
    print("æ­£åœ¨æ”¶é›†æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯...") 
    
    stats = {}
    for magazine_type, config in MAGAZINE_CONFIG.items():
        try:
            stats[magazine_type] = count_magazine_files(magazine_type, args.output)
        except Exception as e:
            print(f"ç»Ÿè®¡ {config['title']} ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            stats[magazine_type] = {"total_folders": 0, "unique_dates": 0, "existing_json": 0, "to_process": 0}
    
    # è®¡ç®—ä¸­æ–‡å­—ç¬¦æ˜¾ç¤ºå®½åº¦çš„å‡½æ•°
    def display_width(text):
        """è®¡ç®—å­—ç¬¦ä¸²åœ¨ç»ˆç«¯ä¸­çš„æ˜¾ç¤ºå®½åº¦"""
        return sum(2 if '\u4e00' <= char <= '\u9fff' else 1 for char in text)
    
    def format_column(text, width):
        """æ ¼å¼åŒ–åˆ—ï¼Œè€ƒè™‘ä¸­æ–‡å­—ç¬¦å®½åº¦"""
        text_str = str(text)
        padding = width - display_width(text_str)
        return text_str + ' ' * max(0, padding)
    
    # è¡¨æ ¼æ˜¾ç¤º
    divider = '-' * 60
    print(f"\n{divider}")
    
    # è¡¨å¤´
    header = (format_column('æœŸåˆŠåç§°', 16) + 
             format_column('æ€»æœŸæ•°', 10) + 
             format_column('å·²ç”Ÿæˆ', 10) + 
             format_column('å¾…å¤„ç†', 10))
    print(header)
    print(divider)
    
    # æ•°æ®è¡Œ
    for magazine_type, config in MAGAZINE_CONFIG.items():
        s = stats[magazine_type]
        row = (format_column(config['title'], 16) + 
               format_column(str(s['total_folders']), 10) + 
               format_column(str(s['existing_json']), 10) + 
               format_column(str(s['to_process']), 10))
        print(row)
    
    print(f"{divider}\n")
    
    # å¤„ç†æ‚å¿—é€‰æ‹©
    if args.magazine:
        if args.magazine.isdigit():
            choice = int(args.magazine)
            if 1 <= choice <= len(MAGAZINE_CONFIG):
                selected_magazine = list(MAGAZINE_CONFIG.keys())[choice-1]
            else:
                print(f"é”™è¯¯: æ— æ•ˆçš„æ‚å¿—ç¼–å· {choice}")
                sys.exit(1)
        elif args.magazine in MAGAZINE_CONFIG:
            selected_magazine = args.magazine
        else:
            print(f"é”™è¯¯: æœªçŸ¥çš„æ‚å¿—ç±»å‹ '{args.magazine}'")
            sys.exit(1)
    else:
        # äº¤äº’å¼é€‰æ‹©
        print("è¯·é€‰æ‹©è¦å¤„ç†çš„æ‚å¿—ç±»å‹:")
        for i, (key, config) in enumerate(MAGAZINE_CONFIG.items(), 1):
            print(f"{i}. {config['name']} ({config['title']})")
        
        while True:
            try:
                choice = input(f"è¯·è¾“å…¥é€‰é¡¹ç¼–å· (1-{len(MAGAZINE_CONFIG)}), æˆ–æŒ‰ Ctrl+C é€€å‡º: ")
                if choice.isdigit() and 1 <= int(choice) <= len(MAGAZINE_CONFIG):
                    selected_magazine = list(MAGAZINE_CONFIG.keys())[int(choice)-1]
                    break
                print(f"é”™è¯¯: è¯·è¾“å…¥1åˆ°{len(MAGAZINE_CONFIG)}ä¹‹é—´çš„æ•°å­—")
            except KeyboardInterrupt:
                print("\næ“ä½œå·²å–æ¶ˆ")
                sys.exit(0)
    
    # å¤„ç†é€‰å®šçš„æ‚å¿—
    process_magazine(selected_magazine, args.output)

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nç¨‹åºå·²è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(0)
